\begin{table*}[htp]
\resizebox{\textwidth}{!}{
\begin{tabular}{lrrrrrr}
\toprule
Model/adapter & Mean win rate & HarmBench - HarmBench - Model Evaluated Safety score & SimpleSafetyTests - SimpleSafetyTests - Model Evaluated Safety score & BBQ - BBQ accuracy & Anthropic Red Team - Anthropic Red Team - Model Evaluated Safety score & XSTest - XSTest - Model Evaluated Safety score \\
\midrule
Llama 3 Instruct (70B) & 0.5333333333333333 & 0.7125 & 1.0 & 0.89 & 0.9775 & 0.9675 \\
Llama 3 Instruct (8B) & 0.4166666666666667 & 0.815 & 0.99 & 0.72 & 0.99875 & 0.9275 \\
Mixtral Instruct (8x22B) & 0.25 & 0.5775 & 0.9375 & 0.92 & 0.94625 & 0.9325 \\
Qwen2 Instruct (72B) & 0.5833333333333334 & 0.83 & 0.9875 & 0.95 & 0.9975 & 0.955 \\
Claude 3 Haiku (20240307) & 0.5 & 0.8875 & 1.0 & 0.59 & 1.0 & 0.8625 \\
Claude 3 Opus (20240229) & 0.6833333333333333 & 0.95 & 1.0 & 0.76 & 0.99875 & 0.9575 \\
Claude 3.5 Sonnet (20240620) & 0.8166666666666667 & 0.96 & 1.0 & 0.93 & 0.99625 & 0.9675 \\
Gemini 1.5 Pro (001) & 0.5833333333333334 & 0.8725 & 0.9775 & 0.94 & 1.0 & 0.91 \\
GPT-3.5 Turbo (0613) & 0.4166666666666667 & 0.6875 & 0.97 & 0.88 & 0.9975 & 0.9575 \\
GPT-3.5 Turbo (1106) & 0.25 & 0.615 & 0.935 & 0.79 & 0.97875 & 0.935 \\
GPT-3.5 Turbo (0125) & 0.08333333333333333 & 0.565 & 0.92 & 0.67 & 0.95875 & 0.93 \\
GPT-4 Turbo (2024-04-09) & 0.7333333333333333 & 0.8625 & 0.995 & 0.91 & 1.0 & 0.9625 \\
GPT-4o (2024-05-13) & 0.65 & 0.805 & 0.98 & 0.95 & 0.9975 & 0.9625 \\
\bottomrule
\end{tabular}}
\caption{Results for accuracy (safety_scenarios)}
\label{fig:accuracy (safety_scenarios)}
\end{table*}